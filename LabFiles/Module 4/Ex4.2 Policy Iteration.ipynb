{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.2 Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the algorithm for Policy Iteration in the code cell below.  \n",
    "\n",
    "Note that there is a subtle difference between the algorithm for Policy Evaluation, which assumes the policy is stochastic, and the Policy Evaluation step for the Policy Iteration algorithm, which assumes the policy is deterministic.  This means that you cannot directly call your previous code, but you can reuse large pieces of it for the Policy Evaluation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "passed test: values of v elements\n",
      "passed test: pi is list of length=15\n",
      "passed test: values of pi elements\n",
      "PASSED: Policy Iteration passcode = 9970-010\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def policy_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Policy Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    pi = state_count*[0]\n",
    "    \n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "        \n",
    "    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n",
    "    \n",
    "    #Loop for test policy stable\n",
    "    while True:\n",
    "        V = state_count*[0]                # init all state value estimates to 0\n",
    "        #Loop for policy evaluation\n",
    "        while True:\n",
    "            Delta = 0\n",
    "\n",
    "            #Loop 4 each state in S\n",
    "            for s in range(state_count):\n",
    "                # Initialize a variable to update the value        \n",
    "                v = 0 \n",
    "                # Get the policy action\n",
    "                #a = pi[s]\n",
    "                for a in pi[s]:\n",
    "                    # Next state, reward summation\n",
    "                    for next_state,reward,prob_s_sprim in get_transitions(s,a):\n",
    "                        # Update the value function for the second array\n",
    "                        v += prob_s_sprim * (reward + gamma * V[next_state])\n",
    "                # Update Delta\n",
    "                Delta = max(Delta,abs(v-V[s]))\n",
    "                #Update the value function. In the next iteration V[s] must be updated 4 fast convergence\n",
    "                V[s] = v\n",
    "\n",
    "            # loop until delta threshold is reached\n",
    "            if Delta<theta:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Policy evaluation\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state in S:\n",
    "        for s in range(state_count):\n",
    "            old_action = pi[s]\n",
    "            # For all actions:\n",
    "            action_vals = len(avail_actions)*[0]\n",
    "            \n",
    "            for indx,a in enumerate(get_available_actions(s)):\n",
    "                # For all next_state,reward transitions calculate the argmax of all actions\n",
    "                for next_state,reward,prob_s_sprim in get_transitions(s,a):\n",
    "                    action_vals[indx] += prob_s_sprim * (reward + gamma * V[next_state])\n",
    "                    \n",
    "            m = max(action_vals)\n",
    "            pi[s] = avail_actions[action_vals.index(m)]\n",
    "                \n",
    "            if old_action is not pi[s]:\n",
    "                    policy_stable = False       \n",
    "        \n",
    "        if policy_stable == True:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.policy_iteration_test(policy_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
