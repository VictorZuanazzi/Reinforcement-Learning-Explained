{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.3 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the algorithm for Value Iteration in the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Value Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "ERROR: v elements don't match expected values: # of mismatches=14\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def value_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Value Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]                # init all state value estimates to 0\n",
    "    pi = state_count*[0]\n",
    "    \n",
    "    # (this section of code can be removed when actual implementation is added)\n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "        \n",
    "        \n",
    "    while True:\n",
    "        Delta = 0\n",
    "        for s in range(state_count):\n",
    "            v = len(get_available_actions(s))*[0]\n",
    "            # For each action to get the action that maximizes the state value\n",
    "            for indx,a in enumerate(get_available_actions(s)):\n",
    "                for next_state,reward,probability in get_transitions(s,a):\n",
    "                    v[indx] += probability * (reward + gamma*V[next_state])\n",
    "            \n",
    "            Delta = max(Delta,abs(max(v)-V[s]))\n",
    "            # Get the max value function\n",
    "            V[s] = max(v)\n",
    "            # Save the policy\n",
    "            pi[s] = avail_actions[v.index(V[s])]\n",
    "        \n",
    "        if Delta<theta:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.value_iteration_test(value_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
